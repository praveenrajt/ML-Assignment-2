import streamlit as st
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score, matthews_corrcoef

# Set page configuration for better layout
st.set_page_config(page_title="Breast Cancer Classification", layout="wide")

st.title("üî¨ ML Assignment 2: Breast Cancer Diagnostic App")
st.markdown("""
This application uses **Machine Learning** to assist in the diagnosis of breast cancer masses. 
It predicts whether a tumor is **Malignant** (Cancerous) or **Benign** (Safe) based on numeric features derived from a digitized image of a fine needle aspirate (FNA).
""")

# --- SIDEBAR ---
st.sidebar.header("‚öôÔ∏è User Input Features")
st.sidebar.info("Upload the 'sample_test_data.csv' generated by your training script to see results.")

# Model Selection
model_name = st.sidebar.selectbox(
    "Select Classification Model",
    ("Logistic_Regression", "Decision_Tree", "KNN", "Naive_Bayes", "Random_Forest", "XGBoost")
)

# Load Assets
try:
    scaler = joblib.load('model/scaler.pkl')
    feature_names = joblib.load('model/features.pkl')
    model = joblib.load(f'model/{model_name}.pkl')
except Exception as e:
    st.error(f"Error loading model files: {e}. Did you run train_models.py?")
    st.stop()

# File Upload
uploaded_file = st.sidebar.file_uploader("Upload CSV file", type=["csv"])

# --- MAIN CONTENT ---

if uploaded_file is not None:
    try:
        input_df = pd.read_csv(uploaded_file)
        
        # Preprocessing
        if 'target' in input_df.columns:
            X_input = input_df.drop(columns=['target'])
            y_true = input_df['target']
            has_labels = True
        else:
            X_input = input_df
            has_labels = False

        # Ensure features match
        X_input = X_input[feature_names]
        
        # Scale if necessary
        if model_name in ["Logistic_Regression", "KNN"]:
            X_processed = scaler.transform(X_input)
        else:
            X_processed = X_input

        # Prediction
        prediction = model.predict(X_processed)
        prediction_proba = model.predict_proba(X_processed)

        # --- RESULTS SECTION ---
        st.divider()
        st.subheader(f"üìä Results using {model_name.replace('_', ' ')}")
        
        # Display raw data (collapsible)
        with st.expander("View Input Data & Predictions"):
            result_df = X_input.copy()
            result_df['Prediction'] = ["Malignant" if p == 0 else "Benign" for p in prediction]
            result_df['Confidence'] = prediction_proba.max(axis=1)
            st.dataframe(result_df)

        if has_labels:
            # Calculate Metrics
            acc = accuracy_score(y_true, prediction)
            prec = precision_score(y_true, prediction)
            rec = recall_score(y_true, prediction)
            f1 = f1_score(y_true, prediction)
            mcc = matthews_corrcoef(y_true, prediction)

            # --- METRICS DASHBOARD ---
            st.subheader("üìà Performance Metrics")
            
            col1, col2, col3, col4, col5 = st.columns(5)
            col1.metric("Accuracy", f"{acc:.2%}")
            col2.metric("Precision", f"{prec:.2%}")
            col3.metric("Recall", f"{rec:.2%}")
            col4.metric("F1 Score", f"{f1:.2%}")
            col5.metric("MCC Score", f"{mcc:.2f}")

            # --- METRICS EXPLANATION (BEAUTIFICATION) ---
            st.info("""
            **‚ÑπÔ∏è Understanding the Metrics:**
            * **Accuracy:** Overall correctness of the model.
            * **Recall (Sensitivity):** Crucial for cancer detection. It measures how many *actual* positive cases (malignant tumors) were correctly identified. A low recall means we missed cancer cases.
            * **Precision:** Measures how many *predicted* positive cases were actually positive. High precision means fewer false alarms.
            * **F1 Score:** The harmonic mean of Precision and Recall. Useful when classes are imbalanced.
            * **MCC (Matthews Correlation Coefficient):** A balanced measure of quality even if classes are of very different sizes. +1 is perfect prediction, 0 is random guessing.
            """)

            # --- VISUALIZATIONS ---
            col_viz1, col_viz2 = st.columns(2)
            
            with col_viz1:
                st.write("**Confusion Matrix**")
                cm = confusion_matrix(y_true, prediction)
                fig, ax = plt.subplots()
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
                plt.ylabel('Actual')
                plt.xlabel('Predicted')
                st.pyplot(fig)

            with col_viz2:
                st.write("**Classification Report**")
                report = classification_report(y_true, prediction, output_dict=True)
                st.dataframe(pd.DataFrame(report).transpose())

    except Exception as e:
        st.error(f"Error processing file. Ensure feature columns match the training data. Details: {e}")

else:
    # --- LANDING PAGE CONTENT ---
    st.subheader("üëã Welcome!")
    st.markdown("""
    To get started, please **upload a CSV file** in the sidebar.
    
    **Instructions:**
    1. Select a model from the dropdown.
    2. Upload your test dataset (must contain 30 numeric feature columns).
    3. The app will automatically generate predictions and performance metrics.
    """)
    
    # Example table of what data looks like
    st.write("Example of expected data format:")
    st.dataframe(pd.DataFrame({
        'mean radius': [17.99, 20.57],
        'mean texture': [10.38, 17.77],
        'mean perimeter': [122.80, 132.90],
        '...': ['...', '...']
    }))